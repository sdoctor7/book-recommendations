{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import math modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import surprise modules\n",
    "from surprise.prediction_algorithms.random_pred import NormalPredictor\n",
    "from surprise.prediction_algorithms.baseline_only import BaselineOnly\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise import evaluate, print_perf\n",
    "from surprise import GridSearch\n",
    "from surprise import accuracy\n",
    "\n",
    "#Graphing\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Setup functions to split data for testing and training\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def pick_users_books(df, num_users, num_books):\n",
    "    #Get the top num_users most prolific users\n",
    "    user_counts = pd.DataFrame(df.user_id.value_counts()).sort_values('user_id', ascending=False)\n",
    "    top_10K_users = list(user_counts[0:num_users].index)\n",
    "    user_filtered_df = df[df.user_id.isin(top_10K_users)]\n",
    "    #Get the top num_books most reviewed books by the selected users\n",
    "    filtered_book_counts = pd.DataFrame(user_filtered_df.book_id.value_counts()).sort_values('book_id', ascending = False)\n",
    "    top_100_filtered_books = list(filtered_book_counts[0:num_books].index)\n",
    "    #Generate new filtered dataframe\n",
    "    filtered_df = user_filtered_df[user_filtered_df.book_id.isin(top_100_filtered_books)]\n",
    "    print(\"New dataframe has {} users, {} items, and a sparsity of {}\".format(len(filtered_df.user_id.unique()),len(filtered_df.book_id.unique()),len(filtered_df)/(len(filtered_df.user_id.unique())*len(filtered_df.book_id.unique()))))\n",
    "    #Split dataframe into training and test sets\n",
    "    train, test = train_test_split(filtered_df, test_size = 0.2, random_state=42)\n",
    "    return train, test\n",
    "\n",
    "def get_all_subsets(df):\n",
    "    #Generate different subsets for scaling purposes\n",
    "    train_500_20, test_500_20 = pick_users_books(df, 500, 20)\n",
    "    train_2000_50, test_2000_50 = pick_users_books(df, 2000, 50)\n",
    "    train_10000_100, test_10000_100 = pick_users_books(df, 10000, 100)\n",
    "\n",
    "    return train_500_20, test_500_20, train_2000_50, test_2000_50, train_10000_100, test_10000_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set up functions to nicely display gridsearch data in a table\n",
    "#Need to do results_df = pd.DataFrame.from_dict(grid_search.cv_results)\n",
    "\n",
    "#Function for alternating least squares\n",
    "def convert_grid_results_als(df):\n",
    "\n",
    "    my_dict = df.to_dict('list')\n",
    "    method = []\n",
    "    reg_i = []\n",
    "    reg_u = []\n",
    "    n_epochs = []\n",
    "    \n",
    "    for x in range(len(my_dict['bsl_options'])):\n",
    "        y = my_dict['bsl_options'][x]\n",
    "        method.append(y['method'])\n",
    "        reg_i.append(y['reg_i'])    \n",
    "        reg_u.append(y['reg_u'])\n",
    "        n_epochs.append(y['n_epochs'])\n",
    "\n",
    "    del my_dict['params']\n",
    "    del my_dict['scores']\n",
    "    del my_dict['bsl_options']\n",
    "\n",
    "    my_dict['method'] = method\n",
    "    my_dict['reg_i'] = reg_i\n",
    "    my_dict['reg_u'] = reg_u\n",
    "    my_dict['n_epochs'] = n_epochs\n",
    "    my_df = pd.DataFrame.from_dict(my_dict).sort_values('RMSE')\n",
    "\n",
    "    return my_df\n",
    "\n",
    "#Function for gradient descent\n",
    "def convert_grid_results_sgd(df):\n",
    "\n",
    "    my_dict = df.to_dict('list')\n",
    "    method = []\n",
    "    reg = []\n",
    "    learning_rate = [] \n",
    "    n_epochs = []\n",
    "\n",
    "    for x in range(len(my_dict['bsl_options'])):\n",
    "        y = my_dict['bsl_options'][x]\n",
    "        method.append(y['method'])\n",
    "        reg.append(y['reg'])        \n",
    "        learning_rate.append(y['learning_rate'])\n",
    "        n_epochs.append(y['n_epochs'])\n",
    "\n",
    "    del my_dict['params']\n",
    "    del my_dict['scores']\n",
    "    del my_dict['bsl_options']\n",
    "\n",
    "    my_dict['method'] = method\n",
    "    my_dict['reg'] = reg    \n",
    "    my_dict['learning_rate'] = learning_rate\n",
    "    my_dict['n_epochs'] = n_epochs\n",
    "    my_df = pd.DataFrame.from_dict(my_dict).sort_values('RMSE')\n",
    "\n",
    "    return my_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load ratings data\n",
    "ratings = pd.read_csv('../ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dataframe has 487 users, 20 items, and a sparsity of 0.44260780287474333\n",
      "New dataframe has 1981 users, 50 items, and a sparsity of 0.3745583038869258\n",
      "New dataframe has 9980 users, 100 items, and a sparsity of 0.2719659318637275\n"
     ]
    }
   ],
   "source": [
    "#Split data into trainint and test sets of different sizes\n",
    "train_500_20, test_500_20, train_2000_50, test_2000_50, train_10000_100, test_10000_100 = get_all_subsets(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run the Normal predictor - no gridsearch needed\n",
    "algo = NormalPredictor()\n",
    "reader = Reader(rating_scale=(1,5))\n",
    "train = Dataset.load_from_df(train_10000_100, reader)\n",
    "test = Dataset.load_from_df(test_10000_100, reader)\n",
    "train.split(n_folds=3)\n",
    "perf = evaluate(algo, train, measures=['RMSE', 'MAE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Grid search for baseline - gradient descent\n",
    "param_grid = {'bsl_options':{'method': ['sgd'], 'reg':[.02,.01,.05], 'learning_rate':[.005,.001, .01], 'n_epochs':[10,20,40]}}\n",
    "gridsearch = GridSearch(BaselineOnly, param_grid, measures=['RMSE', 'MAE'], verbose=0)\n",
    "reader = Reader(rating_scale=(1,5))\n",
    "train = Dataset.load_from_df(train_10000_100, reader)\n",
    "test = Dataset.load_from_df(test_10000_100, reader)\n",
    "train.split(n_folds=3)\n",
    "gridsearch.evaluate(train)\n",
    "convert_grid_results_sgd(pd.DataFrame.from_dict(gridsearch.cv_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Grid search for baseline - alternating least squares\n",
    "param_grid = {'bsl_options':{'method': ['als'], 'reg_i':[5,10,20], 'reg_u':[10,15,20], 'n_epochs':[5,10,20]}}\n",
    "gridsearch = GridSearch(BaselineOnly, param_grid, measures=['RMSE', 'MAE'], verbose=0)\n",
    "reader = Reader(rating_scale=(1,5))\n",
    "train = Dataset.load_from_df(train_500_20, reader)\n",
    "test = Dataset.load_from_df(test_500_20, reader)\n",
    "train.split(n_folds=3)\n",
    "gridsearch.evaluate(train)\n",
    "convert_grid_results_als(pd.DataFrame.from_dict(gridsearch.cv_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating for 500 users and 20 books\n",
      "Estimating biases using sgd...\n",
      "--- 0.08469676971435547 seconds ---\n",
      "RMSE: 0.8418\n",
      "MAE:  0.6653\n",
      "FCP:  0.6223\n",
      "\n",
      "Evaluating for 2000 users and 50 books\n",
      "Estimating biases using sgd...\n",
      "--- 0.8695199489593506 seconds ---\n",
      "RMSE: 0.8432\n",
      "MAE:  0.6586\n",
      "FCP:  0.6421\n",
      "\n",
      "Evaluating for 10000 users and 100 books\n",
      "Estimating biases using sgd...\n",
      "--- 8.371980905532837 seconds ---\n",
      "RMSE: 0.8582\n",
      "MAE:  0.6711\n",
      "FCP:  0.6517\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "#Combine train and test sets\n",
    "all_10000_100 = train_10000_100.append(test_10000_100)\n",
    "all_500_20 = train_500_20.append(test_500_20)\n",
    "all_2000_50 = train_2000_50.append(test_2000_50)\n",
    "\n",
    "def runBaselineSGD(dataframe, num_users, num_books):\n",
    "    print(\"\\nEvaluating for {} users and {} books\".format(num_users, num_books))\n",
    "    \n",
    "    #Capture start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #set up reader and get data from dataframe\n",
    "    reader = Reader(rating_scale=(1, 5))\n",
    "    data = Dataset.load_from_df(dataframe[['user_id', 'book_id', 'rating']], reader)\n",
    "\n",
    "    #Split data into training set, testing set, and the anti-test set (all the items neither in test nor train)\n",
    "    trainset = data.build_full_trainset()\n",
    "    testset = trainset.build_testset()\n",
    "    antitestset = trainset.build_anti_testset()\n",
    "\n",
    "    #Set up algorithm\n",
    "    bsl_options = {'method': 'sgd', 'learning_rate':.005, 'n_epochs':20, 'reg':.01}\n",
    "    algo = BaselineOnly(bsl_options = bsl_options)\n",
    "\n",
    "    #Train algorithm\n",
    "    algo.train(trainset)\n",
    "\n",
    "    #Get predictions\n",
    "    predictions = algo.test(testset)\n",
    "    \n",
    "    #Print algorithm time\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    #Evaluate accuracy measures\n",
    "    accuracy.rmse(predictions, verbose=True)\n",
    "    accuracy.mae(predictions, verbose=True)\n",
    "    accuracy.fcp(predictions, verbose=True)\n",
    "    \n",
    "runBaselineSGD(all_500_20,500,20)\n",
    "runBaselineSGD(all_2000_50,2000,50)\n",
    "runBaselineSGD(all_10000_100,10000,100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
